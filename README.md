# üß† AI-Driven 3D Training Visualization Platform

This repository contains a web-based prototype platform that explores how AI, 3D visualization, and interactive avatars can be combined to support training, learning, and contextual understanding.

The project consists of two prototypes, each building on the previous one to demonstrate increasing levels of interactivity and intelligence.

> **Note:** The focus of this work is clarity, feasibility, and learning, not visual perfection.

---

## üìå Project Overview

The platform demonstrates how:
* **Natural language** can be interpreted into structured actions.
* **3D avatars** can respond to AI-generated commands.
* **Interactive 3D scenes** can enhance training and understanding.

The system is split into two main prototypes:
1. **Prototype 1:** 3D asset visualization & interaction.
2. **Prototype 2:** AI-driven animated training avatar.

---

## üß© Prototype 1 ‚Äì 3D Asset Visualization

### Description
Prototype 1 focuses on rendering and interacting with 3D assets in a web environment. It demonstrates how static or semi-interactive 3D models can be viewed, inspected, and controlled using a browser-based 3D canvas.

### Key Features
* **Web-based 3D rendering** using Three.js.
* **Camera controls** (rotate, zoom, reset).
* **Clean separation** between UI and 3D scene.
* **Foundation** for mapping real-world asset data to 3D representations.

### Purpose
This prototype establishes the visual and technical foundation for later AI-driven interaction.

---

## üßë‚Äçüè´ Prototype 2 ‚Äì AI-Driven Training Avatar

### Description
Prototype 2 introduces a humanoid training avatar that responds to natural language commands interpreted by AI. Users can type commands such as:
* *"Wave hello"*
* *"Point at the extinguisher"*
* *"Jump"*
* *"Walk forward"*
* *"Dance"*

The system interprets these commands and plays the corresponding skeletal animation on the avatar in real time.

### Tool 1: Natural Language Command Interpreter
* Accepts free-form user input.
* Uses AI logic to extract user intent.
* Maps intent to predefined avatar actions.
* Returns a short explanation of the action performed.

### Tool 2: Interactive 3D Avatar Viewer
* Renders a humanoid avatar in a WebGL canvas.
* Plays animations stored in a single GLB file.
* **Supports:** Camera rotation, Zoom in/out, and Scene reset.
* Avatar remains anchored while animations play on the skeleton.

---

## üõ†Ô∏è Technologies Used

### Frontend
* **Next.js** (React)
* **@react-three/fiber**
* **@react-three/drei**
* **Three.js**

### 3D & Animation
* **Blender** ‚Äì Assembling avatar and animations.
* **Mixamo** ‚Äì Humanoid animation source.
* **glTF / GLB** ‚Äì Optimized 3D asset format.

### AI
* **Custom API route** for command interpretation.
* **Natural language ‚Üí action mapping**.
* **Human-readable AI explanations**.

---

## ü§ñ AI Systems Used

### Groq (LLM Inference)
Groq is used to interpret natural language commands and map them to structured avatar actions.

* **Model:** `llama-3.1-8b-instant`
* **Purpose:**
    * Convert free-form user input into predefined action labels.
    * Return a short natural-language explanation of the action.
* **Why Groq:**
    * Extremely fast inference.
    * Low latency, ideal for interactive UIs.
    * Accessed via its OpenAI-compatible API endpoint.

### Imagga (Visual Intelligence & Image Recognition)
Imagga is used for:
* **Automated Image Tagging**: Instantly generating descriptive keywords for uploaded images.
* **Visual Content Analysis**: Extracting meaningful data from pixel content to understand the context of the user's photos.
* **Rapid Prototyping**: Leveraging a stable, production-ready computer vision API to accelerate development.

---

## ‚ö†Ô∏è Known Limitations
* Animations are predefined: No procedural motion (physics-based).
* No physics-based interaction: Limited interaction with objects.
* Limited environmental awareness: The avatar is not aware of surroundings.
* AI interpretation: Lightweight and rule-based for this prototype.
* No voice input: Currently lacks real-time speech recognition.

---

## üîÆ Next Steps & Improvements
* Add a true idle animation state.
* Introduce object-aware interactions.
* Support chained commands (‚ÄúWalk forward and wave‚Äù).
* Add voice-based control.
* Integrate real asset data (e.g. maintenance or training metadata).
* Expand environments beyond a single avatar scene.
